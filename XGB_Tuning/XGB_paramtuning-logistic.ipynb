{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import pickle\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "data= pickle.load(open('/Users/chaitanya/Documents/python/keystrokes/data_augmentation/greyc1/PCA_GREYC_2X.pickle','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x,numlabels):\n",
    "    t = [0 for i in range(numlabels)]\n",
    "    t[x-1] = 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEER_(y_score, y_test):\n",
    "\tn_classes = y_score.shape[1]\n",
    "\tfpr = dict()\n",
    "\ttpr = dict()\n",
    "\troc_auc = dict()\n",
    "\tmissRate = dict()\n",
    "\tfor i in range(n_classes):\n",
    "\t\tfpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "\t\troc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\tfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "\tmissRate[\"micro\"] = 1 - tpr[\"micro\"]\n",
    "\tall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\t# Then interpolate all ROC curves at this points\n",
    "\tmean_tpr = np.zeros_like(all_fpr)\n",
    "\tfor i in range(n_classes):\n",
    "\t\tmean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\tmean_tpr /= n_classes\n",
    "\tfpr[\"macro\"] = all_fpr\n",
    "\ttpr[\"macro\"] = mean_tpr\n",
    "\tmissRate[\"macro\"] = 1 - tpr[\"macro\"]\n",
    "\treturn  min(fpr[\"micro\"][np.argmin(abs(fpr[\"micro\"]-missRate[\"micro\"]))], fpr[\"macro\"][np.argmin(abs(fpr[\"macro\"]-missRate[\"macro\"]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEER(y_score, y_test):\n",
    "    #y_score = y_score.get_label()\n",
    "    y_test = np.array(y_test.get_label()).reshape(len(y_score), 100)\n",
    "    n_classes = y_score.shape[1]\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    missRate = dict()\n",
    "    #y_test_ = [onehot(int(x+1), 100) for x in y_test]\n",
    "    #y_test = np.reshape(y_test_, (len(y_test_),100))\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    missRate[\"micro\"] = 1 - tpr[\"micro\"]\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    missRate[\"macro\"] = 1 - tpr[\"macro\"]\n",
    "    #print(np.dtype(min(fpr[\"micro\"][np.argmin(abs(fpr[\"micro\"]-missRate[\"micro\"]))], fpr[\"macro\"][np.argmin(abs(fpr[\"macro\"]-missRate[\"macro\"]))])))\n",
    "    return 'EER', min(fpr[\"micro\"][np.argmin(abs(fpr[\"micro\"]-missRate[\"micro\"]))], fpr[\"macro\"][np.argmin(abs(fpr[\"macro\"]-missRate[\"macro\"]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEER_CV(y_score, y_test):\n",
    "    #y_score = y_score.get_label()\n",
    "    y_test = np.array(y_test.get_label()).reshape(len(y_score), 100)\n",
    "    n_classes = y_score.shape[1]\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    missRate = dict()\n",
    "    y_test_ = [onehot(int(x+1), 100) for x in y_test]\n",
    "    y_score = np.reshape(y_score,(len(y_test_),100))\n",
    "    y_test = np.reshape(y_test_, (len(y_test_),100))\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    missRate[\"micro\"] = 1 - tpr[\"micro\"]\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    missRate[\"macro\"] = 1 - tpr[\"macro\"]\n",
    "    #print(np.dtype(min(fpr[\"micro\"][np.argmin(abs(fpr[\"micro\"]-missRate[\"micro\"]))], fpr[\"macro\"][np.argmin(abs(fpr[\"macro\"]-missRate[\"macro\"]))])))\n",
    "    return 'EER', min(fpr[\"micro\"][np.argmin(abs(fpr[\"micro\"]-missRate[\"micro\"]))], fpr[\"macro\"][np.argmin(abs(fpr[\"macro\"]-missRate[\"macro\"]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2223, 2223, 12593, 12593]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1259300, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['data']\n",
    "X = np.array(X).reshape(len(X), 28)\n",
    "y = np.array(data['labels']).reshape(len(X), 100)\n",
    "#X = X[:100,:]\n",
    "#y = y[:100,:]\n",
    "#y_train = [onehot(x,100) for x in train_data['labels']]\n",
    "#X_test = np.array(test_data['features']).reshape(len(test_data['features']), 1, 28, 1)\n",
    "#y_test = [onehot(x,100) for x in test_data['labels']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "#X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=122)\n",
    "print([len(x) for x in [X_test, y_test, X_train, y_train]])\n",
    "\n",
    "y_train=np.reshape(y_train,(len(y_train)*100,1))\n",
    "y_test = np.reshape(y_test, (len(y_test)*100,1))\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(X_train, label = y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label = y_test)\n",
    "xg_train_cv = xgb.DMatrix(X_train, label = np.reshape(y_train,(12593,100)))\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':10,\n",
    "    'eval_metric': 'rmse',\n",
    "    'min_child_weight': 8,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:logistic',\n",
    "    'num_class' : 100,\n",
    "}\n",
    "num_boost_round = 999\n",
    "params['tree_method']= 'hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-rmse:0.451199\tTest-EER:0.164529\n",
      "Multiple eval metrics have been passed: 'Test-EER' will be used for early stopping.\n",
      "\n",
      "Will train until Test-EER hasn't improved in 20 rounds.\n",
      "[1]\tTest-rmse:0.407714\tTest-EER:0.138947\n",
      "[2]\tTest-rmse:0.368858\tTest-EER:0.126747\n",
      "[3]\tTest-rmse:0.334078\tTest-EER:0.121944\n",
      "[4]\tTest-rmse:0.302883\tTest-EER:0.112538\n",
      "[5]\tTest-rmse:0.2749\tTest-EER:0.107072\n",
      "[6]\tTest-rmse:0.249794\tTest-EER:0.101696\n",
      "[7]\tTest-rmse:0.227298\tTest-EER:0.096957\n",
      "[8]\tTest-rmse:0.207124\tTest-EER:0.095117\n",
      "[9]\tTest-rmse:0.18907\tTest-EER:0.092195\n",
      "[10]\tTest-rmse:0.172856\tTest-EER:0.087392\n",
      "[11]\tTest-rmse:0.158367\tTest-EER:0.085011\n",
      "[12]\tTest-rmse:0.145419\tTest-EER:0.083185\n",
      "[13]\tTest-rmse:0.133907\tTest-EER:0.080426\n",
      "[14]\tTest-rmse:0.123631\tTest-EER:0.076991\n",
      "[15]\tTest-rmse:0.114513\tTest-EER:0.076455\n",
      "[16]\tTest-rmse:0.106403\tTest-EER:0.07584\n",
      "[17]\tTest-rmse:0.099269\tTest-EER:0.072793\n",
      "[18]\tTest-rmse:0.092978\tTest-EER:0.068353\n",
      "[19]\tTest-rmse:0.087458\tTest-EER:0.066081\n",
      "[20]\tTest-rmse:0.08264\tTest-EER:0.06455\n",
      "[21]\tTest-rmse:0.078418\tTest-EER:0.063752\n",
      "[22]\tTest-rmse:0.074795\tTest-EER:0.063888\n",
      "[23]\tTest-rmse:0.071587\tTest-EER:0.062524\n",
      "[24]\tTest-rmse:0.068814\tTest-EER:0.061188\n",
      "[25]\tTest-rmse:0.066419\tTest-EER:0.060706\n",
      "[26]\tTest-rmse:0.064396\tTest-EER:0.060261\n",
      "[27]\tTest-rmse:0.062575\tTest-EER:0.058829\n",
      "[28]\tTest-rmse:0.06101\tTest-EER:0.057603\n",
      "[29]\tTest-rmse:0.059681\tTest-EER:0.05668\n",
      "[30]\tTest-rmse:0.058621\tTest-EER:0.056662\n",
      "[31]\tTest-rmse:0.057634\tTest-EER:0.053536\n",
      "[32]\tTest-rmse:0.056781\tTest-EER:0.053172\n",
      "[33]\tTest-rmse:0.056012\tTest-EER:0.052214\n",
      "[34]\tTest-rmse:0.055348\tTest-EER:0.051577\n",
      "[35]\tTest-rmse:0.054778\tTest-EER:0.051282\n",
      "[36]\tTest-rmse:0.054324\tTest-EER:0.051155\n",
      "[37]\tTest-rmse:0.05389\tTest-EER:0.051073\n",
      "[38]\tTest-rmse:0.053496\tTest-EER:0.05\n",
      "[39]\tTest-rmse:0.053165\tTest-EER:0.049714\n",
      "[40]\tTest-rmse:0.052895\tTest-EER:0.049528\n",
      "[41]\tTest-rmse:0.05263\tTest-EER:0.048906\n",
      "[42]\tTest-rmse:0.052364\tTest-EER:0.047511\n",
      "[43]\tTest-rmse:0.05209\tTest-EER:0.046839\n",
      "[44]\tTest-rmse:0.051859\tTest-EER:0.046448\n",
      "[45]\tTest-rmse:0.051685\tTest-EER:0.045888\n",
      "[46]\tTest-rmse:0.051498\tTest-EER:0.045847\n",
      "[47]\tTest-rmse:0.051357\tTest-EER:0.045144\n",
      "[48]\tTest-rmse:0.051211\tTest-EER:0.044384\n",
      "[49]\tTest-rmse:0.051075\tTest-EER:0.044171\n",
      "[50]\tTest-rmse:0.050954\tTest-EER:0.044091\n",
      "[51]\tTest-rmse:0.050828\tTest-EER:0.043976\n",
      "[52]\tTest-rmse:0.050692\tTest-EER:0.04323\n",
      "[53]\tTest-rmse:0.050527\tTest-EER:0.042631\n",
      "[54]\tTest-rmse:0.05042\tTest-EER:0.042235\n",
      "[55]\tTest-rmse:0.050305\tTest-EER:0.041136\n",
      "[56]\tTest-rmse:0.050203\tTest-EER:0.041685\n",
      "[57]\tTest-rmse:0.050114\tTest-EER:0.041648\n",
      "[58]\tTest-rmse:0.050019\tTest-EER:0.041804\n",
      "[59]\tTest-rmse:0.049914\tTest-EER:0.04\n",
      "[60]\tTest-rmse:0.049805\tTest-EER:0.04039\n",
      "[61]\tTest-rmse:0.049743\tTest-EER:0.040054\n",
      "[62]\tTest-rmse:0.049653\tTest-EER:0.03935\n",
      "[63]\tTest-rmse:0.049571\tTest-EER:0.039177\n",
      "[64]\tTest-rmse:0.049466\tTest-EER:0.038479\n",
      "[65]\tTest-rmse:0.049391\tTest-EER:0.037932\n",
      "[66]\tTest-rmse:0.049294\tTest-EER:0.038128\n",
      "[67]\tTest-rmse:0.049221\tTest-EER:0.036942\n",
      "[68]\tTest-rmse:0.049167\tTest-EER:0.036868\n",
      "[69]\tTest-rmse:0.04911\tTest-EER:0.037642\n",
      "[70]\tTest-rmse:0.049017\tTest-EER:0.036987\n",
      "[71]\tTest-rmse:0.04895\tTest-EER:0.03736\n",
      "[72]\tTest-rmse:0.048915\tTest-EER:0.036428\n",
      "[73]\tTest-rmse:0.048834\tTest-EER:0.036156\n",
      "[74]\tTest-rmse:0.048779\tTest-EER:0.034974\n",
      "[75]\tTest-rmse:0.048719\tTest-EER:0.035369\n",
      "[76]\tTest-rmse:0.04865\tTest-EER:0.03506\n",
      "[77]\tTest-rmse:0.048593\tTest-EER:0.034252\n",
      "[78]\tTest-rmse:0.048539\tTest-EER:0.034134\n",
      "[79]\tTest-rmse:0.04847\tTest-EER:0.034079\n",
      "[80]\tTest-rmse:0.048422\tTest-EER:0.034134\n",
      "[81]\tTest-rmse:0.048361\tTest-EER:0.033488\n",
      "[82]\tTest-rmse:0.048301\tTest-EER:0.033825\n",
      "[83]\tTest-rmse:0.048251\tTest-EER:0.033443\n",
      "[84]\tTest-rmse:0.048223\tTest-EER:0.033702\n",
      "[85]\tTest-rmse:0.048172\tTest-EER:0.032739\n",
      "[86]\tTest-rmse:0.048136\tTest-EER:0.03347\n",
      "[87]\tTest-rmse:0.048095\tTest-EER:0.033838\n",
      "[88]\tTest-rmse:0.04804\tTest-EER:0.033829\n",
      "[89]\tTest-rmse:0.048031\tTest-EER:0.032861\n",
      "[90]\tTest-rmse:0.047996\tTest-EER:0.032725\n",
      "[91]\tTest-rmse:0.047963\tTest-EER:0.032725\n",
      "[92]\tTest-rmse:0.047946\tTest-EER:0.032757\n",
      "[93]\tTest-rmse:0.047931\tTest-EER:0.032348\n",
      "[94]\tTest-rmse:0.047886\tTest-EER:0.03238\n",
      "[95]\tTest-rmse:0.047863\tTest-EER:0.032202\n",
      "[96]\tTest-rmse:0.047813\tTest-EER:0.032775\n",
      "[97]\tTest-rmse:0.047786\tTest-EER:0.032725\n",
      "[98]\tTest-rmse:0.047768\tTest-EER:0.032611\n",
      "[99]\tTest-rmse:0.047738\tTest-EER:0.033029\n",
      "[100]\tTest-rmse:0.047713\tTest-EER:0.032802\n",
      "[101]\tTest-rmse:0.047682\tTest-EER:0.033357\n",
      "[102]\tTest-rmse:0.047653\tTest-EER:0.032389\n",
      "[103]\tTest-rmse:0.047626\tTest-EER:0.032439\n",
      "[104]\tTest-rmse:0.047601\tTest-EER:0.033066\n",
      "[105]\tTest-rmse:0.047565\tTest-EER:0.032484\n",
      "[106]\tTest-rmse:0.047543\tTest-EER:0.033147\n",
      "[107]\tTest-rmse:0.04751\tTest-EER:0.032221\n",
      "[108]\tTest-rmse:0.047493\tTest-EER:0.032461\n",
      "[109]\tTest-rmse:0.047484\tTest-EER:0.032643\n",
      "[110]\tTest-rmse:0.047464\tTest-EER:0.032493\n",
      "[111]\tTest-rmse:0.04745\tTest-EER:0.032325\n",
      "[112]\tTest-rmse:0.04743\tTest-EER:0.032311\n",
      "[113]\tTest-rmse:0.047414\tTest-EER:0.032021\n",
      "[114]\tTest-rmse:0.047399\tTest-EER:0.031675\n",
      "[115]\tTest-rmse:0.047385\tTest-EER:0.03233\n",
      "[116]\tTest-rmse:0.047367\tTest-EER:0.031207\n",
      "[117]\tTest-rmse:0.047336\tTest-EER:0.031693\n",
      "[118]\tTest-rmse:0.047322\tTest-EER:0.031266\n",
      "[119]\tTest-rmse:0.047313\tTest-EER:0.031962\n",
      "[120]\tTest-rmse:0.047308\tTest-EER:0.031698\n",
      "[121]\tTest-rmse:0.047293\tTest-EER:0.032175\n",
      "[122]\tTest-rmse:0.047279\tTest-EER:0.032089\n",
      "[123]\tTest-rmse:0.047254\tTest-EER:0.032452\n",
      "[124]\tTest-rmse:0.047259\tTest-EER:0.032207\n",
      "[125]\tTest-rmse:0.047243\tTest-EER:0.03208\n",
      "[126]\tTest-rmse:0.047224\tTest-EER:0.031957\n",
      "[127]\tTest-rmse:0.047215\tTest-EER:0.032071\n",
      "[128]\tTest-rmse:0.047208\tTest-EER:0.031203\n",
      "[129]\tTest-rmse:0.047183\tTest-EER:0.03168\n",
      "[130]\tTest-rmse:0.047153\tTest-EER:0.031421\n",
      "[131]\tTest-rmse:0.047155\tTest-EER:0.031421\n",
      "[132]\tTest-rmse:0.047148\tTest-EER:0.030994\n",
      "[133]\tTest-rmse:0.047133\tTest-EER:0.031421\n",
      "[134]\tTest-rmse:0.047134\tTest-EER:0.031353\n",
      "[135]\tTest-rmse:0.047143\tTest-EER:0.031748\n",
      "[136]\tTest-rmse:0.047123\tTest-EER:0.031462\n",
      "[137]\tTest-rmse:0.047109\tTest-EER:0.031793\n",
      "[138]\tTest-rmse:0.047098\tTest-EER:0.03173\n",
      "[139]\tTest-rmse:0.047097\tTest-EER:0.031675\n",
      "[140]\tTest-rmse:0.047097\tTest-EER:0.030862\n",
      "[141]\tTest-rmse:0.047089\tTest-EER:0.031634\n",
      "[142]\tTest-rmse:0.047086\tTest-EER:0.031366\n",
      "[143]\tTest-rmse:0.047069\tTest-EER:0.031893\n",
      "[144]\tTest-rmse:0.047059\tTest-EER:0.031353\n",
      "[145]\tTest-rmse:0.047052\tTest-EER:0.030394\n",
      "[146]\tTest-rmse:0.047052\tTest-EER:0.031421\n",
      "[147]\tTest-rmse:0.047034\tTest-EER:0.031689\n",
      "[148]\tTest-rmse:0.047018\tTest-EER:0.031884\n",
      "[149]\tTest-rmse:0.047015\tTest-EER:0.031116\n",
      "[150]\tTest-rmse:0.047007\tTest-EER:0.031053\n",
      "[151]\tTest-rmse:0.047006\tTest-EER:0.031248\n",
      "[152]\tTest-rmse:0.046986\tTest-EER:0.031525\n",
      "[153]\tTest-rmse:0.046973\tTest-EER:0.031098\n",
      "[154]\tTest-rmse:0.046961\tTest-EER:0.030535\n",
      "[155]\tTest-rmse:0.046956\tTest-EER:0.031003\n",
      "[156]\tTest-rmse:0.046955\tTest-EER:0.031194\n",
      "[157]\tTest-rmse:0.046952\tTest-EER:0.030789\n",
      "[158]\tTest-rmse:0.04694\tTest-EER:0.031275\n",
      "[159]\tTest-rmse:0.046939\tTest-EER:0.031189\n",
      "[160]\tTest-rmse:0.046935\tTest-EER:0.031516\n",
      "[161]\tTest-rmse:0.046947\tTest-EER:0.031416\n",
      "[162]\tTest-rmse:0.046949\tTest-EER:0.031362\n",
      "[163]\tTest-rmse:0.046941\tTest-EER:0.03113\n",
      "[164]\tTest-rmse:0.046944\tTest-EER:0.030948\n",
      "[165]\tTest-rmse:0.046931\tTest-EER:0.031175\n",
      "Stopping. Best iteration:\n",
      "[145]\tTest-rmse:0.047052\tTest-EER:0.030394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    xg_train,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(xg_test, \"Test\")],\n",
    "    early_stopping_rounds=20,\n",
    "    feval = GetEER,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = np.reshape(xg_train_cv.get_label(),(12593,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259300,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(xg_train_cv.get_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "b'[13:59:00] src/objective/regression_obj.cc:44: Check failed: preds->Size() == info.labels_.size() (1007400 vs. 10074) labels are not correctly providedpreds.size=1007400, label.size=10074\\n\\nStack trace returned 2 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x000000010ac09683 dmlc::StackTrace[abi:cxx11]() + 67\\n[bt] (1) 1   libstdc++.6.dylib                   0x000000010b088ce0 vtable for std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> > + 16\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-36403f39913b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetEER_CV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    404\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 894\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[13:59:00] src/objective/regression_obj.cc:44: Check failed: preds->Size() == info.labels_.size() (1007400 vs. 10074) labels are not correctly providedpreds.size=1007400, label.size=10074\\n\\nStack trace returned 2 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x000000010ac09683 dmlc::StackTrace[abi:cxx11]() + 67\\n[bt] (1) 1   libstdc++.6.dylib                   0x000000010b088ce0 vtable for std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> > + 16\\n\\n'"
     ]
    }
   ],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    xg_train_cv,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'rmse'},\n",
    "    early_stopping_rounds=10,\n",
    "    feval = GetEER_CV,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.219706</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.302707</td>\n",
       "      <td>0.011303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166938</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.285238</td>\n",
       "      <td>0.009576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126916</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.264591</td>\n",
       "      <td>0.008174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.104324</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.246089</td>\n",
       "      <td>0.006187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089216</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.234574</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.073910</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.226554</td>\n",
       "      <td>0.008428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.060073</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.217978</td>\n",
       "      <td>0.008516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046454</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.213769</td>\n",
       "      <td>0.007248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.034960</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.209640</td>\n",
       "      <td>0.006373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026086</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.204717</td>\n",
       "      <td>0.006018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.018681</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.199079</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013142</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.196220</td>\n",
       "      <td>0.006871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009013</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.191614</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.188756</td>\n",
       "      <td>0.007779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.186373</td>\n",
       "      <td>0.007685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.184468</td>\n",
       "      <td>0.008339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.181927</td>\n",
       "      <td>0.008496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.008366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.178750</td>\n",
       "      <td>0.008717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.177162</td>\n",
       "      <td>0.008204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.175097</td>\n",
       "      <td>0.009080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.173827</td>\n",
       "      <td>0.009222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.172635</td>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.009627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.169935</td>\n",
       "      <td>0.009682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.169697</td>\n",
       "      <td>0.009840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.167792</td>\n",
       "      <td>0.009855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.166918</td>\n",
       "      <td>0.009635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.165489</td>\n",
       "      <td>0.009134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164854</td>\n",
       "      <td>0.008614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150957</td>\n",
       "      <td>0.008020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151037</td>\n",
       "      <td>0.007781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150640</td>\n",
       "      <td>0.007797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>0.007926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150640</td>\n",
       "      <td>0.008080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150322</td>\n",
       "      <td>0.008176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150084</td>\n",
       "      <td>0.008108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150004</td>\n",
       "      <td>0.007608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150004</td>\n",
       "      <td>0.007801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150322</td>\n",
       "      <td>0.007977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150163</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150084</td>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150004</td>\n",
       "      <td>0.007596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149607</td>\n",
       "      <td>0.007874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149766</td>\n",
       "      <td>0.007850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150084</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>0.008072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149766</td>\n",
       "      <td>0.008307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149687</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149687</td>\n",
       "      <td>0.008086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149687</td>\n",
       "      <td>0.007957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149528</td>\n",
       "      <td>0.007775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149369</td>\n",
       "      <td>0.008007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149210</td>\n",
       "      <td>0.008030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148972</td>\n",
       "      <td>0.008213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148893</td>\n",
       "      <td>0.008123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148813</td>\n",
       "      <td>0.008120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148813</td>\n",
       "      <td>0.008357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148416</td>\n",
       "      <td>0.008034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148178</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-merror-mean  train-merror-std  test-merror-mean  test-merror-std\n",
       "0             0.219706          0.001887          0.302707         0.011303\n",
       "1             0.166938          0.002620          0.285238         0.009576\n",
       "2             0.126916          0.001585          0.264591         0.008174\n",
       "3             0.104324          0.002049          0.246089         0.006187\n",
       "4             0.089216          0.002075          0.234574         0.006100\n",
       "5             0.073910          0.001888          0.226554         0.008428\n",
       "6             0.060073          0.001529          0.217978         0.008516\n",
       "7             0.046454          0.001211          0.213769         0.007248\n",
       "8             0.034960          0.000816          0.209640         0.006373\n",
       "9             0.026086          0.001223          0.204717         0.006018\n",
       "10            0.018681          0.000846          0.199079         0.005798\n",
       "11            0.013142          0.000853          0.196220         0.006871\n",
       "12            0.009013          0.000680          0.191614         0.007060\n",
       "13            0.006154          0.000494          0.188756         0.007779\n",
       "14            0.004090          0.000553          0.186373         0.007685\n",
       "15            0.002779          0.000461          0.184468         0.008339\n",
       "16            0.001886          0.000355          0.181927         0.008496\n",
       "17            0.001191          0.000108          0.180100         0.008366\n",
       "18            0.000854          0.000173          0.178750         0.008717\n",
       "19            0.000675          0.000146          0.177162         0.008204\n",
       "20            0.000357          0.000048          0.175097         0.009080\n",
       "21            0.000239          0.000048          0.173827         0.009222\n",
       "22            0.000159          0.000080          0.172635         0.008950\n",
       "23            0.000099          0.000000          0.171524         0.009627\n",
       "24            0.000079          0.000040          0.169935         0.009682\n",
       "25            0.000020          0.000040          0.169697         0.009840\n",
       "26            0.000020          0.000040          0.167792         0.009855\n",
       "27            0.000020          0.000040          0.166918         0.009635\n",
       "28            0.000020          0.000040          0.165489         0.009134\n",
       "29            0.000000          0.000000          0.164854         0.008614\n",
       "..                 ...               ...               ...              ...\n",
       "72            0.000000          0.000000          0.150957         0.008020\n",
       "73            0.000000          0.000000          0.151037         0.007781\n",
       "74            0.000000          0.000000          0.150640         0.007797\n",
       "75            0.000000          0.000000          0.150560         0.007926\n",
       "76            0.000000          0.000000          0.150640         0.008080\n",
       "77            0.000000          0.000000          0.150322         0.008176\n",
       "78            0.000000          0.000000          0.150084         0.008108\n",
       "79            0.000000          0.000000          0.150004         0.007608\n",
       "80            0.000000          0.000000          0.150004         0.007801\n",
       "81            0.000000          0.000000          0.150322         0.007977\n",
       "82            0.000000          0.000000          0.150163         0.008000\n",
       "83            0.000000          0.000000          0.150084         0.007831\n",
       "84            0.000000          0.000000          0.150004         0.007596\n",
       "85            0.000000          0.000000          0.149607         0.007874\n",
       "86            0.000000          0.000000          0.149766         0.007850\n",
       "87            0.000000          0.000000          0.150084         0.007888\n",
       "88            0.000000          0.000000          0.150243         0.008072\n",
       "89            0.000000          0.000000          0.149766         0.008307\n",
       "90            0.000000          0.000000          0.149687         0.007832\n",
       "91            0.000000          0.000000          0.149687         0.008086\n",
       "92            0.000000          0.000000          0.149687         0.007957\n",
       "93            0.000000          0.000000          0.149528         0.007775\n",
       "94            0.000000          0.000000          0.149369         0.008007\n",
       "95            0.000000          0.000000          0.149210         0.008030\n",
       "96            0.000000          0.000000          0.148972         0.008213\n",
       "97            0.000000          0.000000          0.148893         0.008123\n",
       "98            0.000000          0.000000          0.148813         0.008120\n",
       "99            0.000000          0.000000          0.148813         0.008357\n",
       "100           0.000000          0.000000          0.148416         0.008034\n",
       "101           0.000000          0.000000          0.148178         0.008186\n",
       "\n",
       "[102 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "b'[13:26:47] src/objective/regression_obj.cc:44: Check failed: preds->Size() == info.labels_.size() (1007400 vs. 10074) labels are not correctly providedpreds.size=1007400, label.size=10074\\n\\nStack trace returned 2 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x000000010ac09683 dmlc::StackTrace[abi:cxx11]() + 67\\n[bt] (1) 1   libstdc++.6.dylib                   0x000000010b088ce0 vtable for std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> > + 16\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d041b59ecc57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetEER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    404\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 894\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[13:26:47] src/objective/regression_obj.cc:44: Check failed: preds->Size() == info.labels_.size() (1007400 vs. 10074) labels are not correctly providedpreds.size=1007400, label.size=10074\\n\\nStack trace returned 2 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x000000010ac09683 dmlc::StackTrace[abi:cxx11]() + 67\\n[bt] (1) 1   libstdc++.6.dylib                   0x000000010b088ce0 vtable for std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> > + 16\\n\\n'"
     ]
    }
   ],
   "source": [
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        xg_train,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'rmse'},\n",
    "        early_stopping_rounds=10,\n",
    "        feval = GetEER\n",
    "    )\n",
    "\n",
    "    # Update best MAE\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_merror\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Merror: {}\".format(best_params[0], best_params[1], min_merror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=10, min_child_weight=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMerror 0.1170488 for 71 rounds\n",
      "CV with max_depth=10, min_child_weight=8\n",
      "\tMerror 0.1141906 for 93 rounds\n",
      "CV with max_depth=10, min_child_weight=9\n",
      "\tMerror 0.1146672 for 76 rounds\n",
      "CV with max_depth=10, min_child_weight=10\n",
      "\tMerror 0.1146674 for 61 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tMerror 0.1170488 for 71 rounds\n",
      "CV with max_depth=11, min_child_weight=8\n",
      "\tMerror 0.1141906 for 93 rounds\n",
      "CV with max_depth=11, min_child_weight=9\n",
      "\tMerror 0.1146672 for 76 rounds\n",
      "CV with max_depth=11, min_child_weight=10\n",
      "\tMerror 0.1146674 for 61 rounds\n",
      "CV with max_depth=12, min_child_weight=7\n",
      "\tMerror 0.1170488 for 71 rounds\n",
      "CV with max_depth=12, min_child_weight=8\n",
      "\tMerror 0.1141906 for 93 rounds\n",
      "CV with max_depth=12, min_child_weight=9\n",
      "\tMerror 0.1146672 for 76 rounds\n",
      "CV with max_depth=12, min_child_weight=10\n",
      "\tMerror 0.1146674 for 61 rounds\n",
      "Best params: 10, 8, Merror: 0.1141906\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(10,13)\n",
    "    for min_child_weight in range(7,11)\n",
    "]\n",
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        xg_train,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'merror'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best MAE\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_merror\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Merror: {}\".format(best_params[0], best_params[1], min_merror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 10\n",
    "params['min_child_weight'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMerror 0.1141906 for 93 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "\tMerror 0.1158584 for 61 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tMerror 0.1166528 for 66 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "\tMerror 0.1137942 for 75 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tMerror 0.11403160000000001 for 48 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tMerror 0.11506419999999999 for 46 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tMerror 0.11387320000000001 for 72 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tMerror 0.1138738 for 89 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tMerror 0.11220539999999998 for 102 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tMerror 0.1123644 for 77 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tMerror 0.11236439999999999 for 77 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tMerror 0.1150646 for 64 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "\tMerror 0.114429 for 70 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tMerror 0.11514359999999998 for 47 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tMerror 0.11482620000000002 for 53 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tMerror 0.1156204 for 46 rounds\n",
      "Best params: 0.8, 1.0, Merror: 0.11220539999999998\n"
     ]
    }
   ],
   "source": [
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        xg_train,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'merror'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_merror\n",
    "        best_params = (subsample,colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, Merror: {}\".format(best_params[0], best_params[1], min_merror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "CPU times: user 6 Âµs, sys: 1 Âµs, total: 7 Âµs\n",
      "Wall time: 14.1 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMerror 0.11220539999999998 for 102 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "CPU times: user 5 Âµs, sys: 1e+03 ns, total: 6 Âµs\n",
      "Wall time: 12.2 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMerror 0.11037919999999998 for 79 rounds\n",
      "\n",
      "CV with eta=0.1\n",
      "CPU times: user 5 Âµs, sys: 1e+03 ns, total: 6 Âµs\n",
      "Wall time: 10 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMerror 0.11093519999999998 for 140 rounds\n",
      "\n",
      "CV with eta=0.05\n",
      "CPU times: user 5 Âµs, sys: 0 ns, total: 5 Âµs\n",
      "Wall time: 11.9 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMerror 0.10990300000000001 for 245 rounds\n",
      "\n",
      "CV with eta=0.01\n",
      "CPU times: user 5 Âµs, sys: 1 Âµs, total: 6 Âµs\n",
      "Wall time: 12.2 Âµs\n",
      "\tMerror 0.13563119999999998 for 403 rounds\n",
      "\n",
      "Best params: 0.05, Merror: 0.12506940000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    }
   ],
   "source": [
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    %time\n",
    "    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "            params,\n",
    "            xg_train,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics=['merror'],\n",
    "            early_stopping_rounds=10\n",
    "          )\n",
    "\n",
    "    # Update best score\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\\n\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_mae\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, Merror: {}\".format(best_params, min_merror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-merror:0.299595\n",
      "Will train until Test-merror hasn't improved in 20 rounds.\n",
      "[1]\tTest-merror:0.249663\n",
      "[2]\tTest-merror:0.234818\n",
      "[3]\tTest-merror:0.230769\n",
      "[4]\tTest-merror:0.217274\n",
      "[5]\tTest-merror:0.211426\n",
      "[6]\tTest-merror:0.209177\n",
      "[7]\tTest-merror:0.203329\n",
      "[8]\tTest-merror:0.19928\n",
      "[9]\tTest-merror:0.194782\n",
      "[10]\tTest-merror:0.196131\n",
      "[11]\tTest-merror:0.194332\n",
      "[12]\tTest-merror:0.189834\n",
      "[13]\tTest-merror:0.185785\n",
      "[14]\tTest-merror:0.183086\n",
      "[15]\tTest-merror:0.180387\n",
      "[16]\tTest-merror:0.178587\n",
      "[17]\tTest-merror:0.175888\n",
      "[18]\tTest-merror:0.175888\n",
      "[19]\tTest-merror:0.175888\n",
      "[20]\tTest-merror:0.176788\n",
      "[21]\tTest-merror:0.174989\n",
      "[22]\tTest-merror:0.173639\n",
      "[23]\tTest-merror:0.173189\n",
      "[24]\tTest-merror:0.17049\n",
      "[25]\tTest-merror:0.169141\n",
      "[26]\tTest-merror:0.166442\n",
      "[27]\tTest-merror:0.165092\n",
      "[28]\tTest-merror:0.164193\n",
      "[29]\tTest-merror:0.163743\n",
      "[30]\tTest-merror:0.162843\n",
      "[31]\tTest-merror:0.161044\n",
      "[32]\tTest-merror:0.160594\n",
      "[33]\tTest-merror:0.160594\n",
      "[34]\tTest-merror:0.159694\n",
      "[35]\tTest-merror:0.158345\n",
      "[36]\tTest-merror:0.158345\n",
      "[37]\tTest-merror:0.157445\n",
      "[38]\tTest-merror:0.156095\n",
      "[39]\tTest-merror:0.154746\n",
      "[40]\tTest-merror:0.155196\n",
      "[41]\tTest-merror:0.152047\n",
      "[42]\tTest-merror:0.152497\n",
      "[43]\tTest-merror:0.152497\n",
      "[44]\tTest-merror:0.151597\n",
      "[45]\tTest-merror:0.149798\n",
      "[46]\tTest-merror:0.148898\n",
      "[47]\tTest-merror:0.147548\n",
      "[48]\tTest-merror:0.147099\n",
      "[49]\tTest-merror:0.147099\n",
      "[50]\tTest-merror:0.147099\n",
      "[51]\tTest-merror:0.147099\n",
      "[52]\tTest-merror:0.147099\n",
      "[53]\tTest-merror:0.147548\n",
      "[54]\tTest-merror:0.146649\n",
      "[55]\tTest-merror:0.146199\n",
      "[56]\tTest-merror:0.145749\n",
      "[57]\tTest-merror:0.145299\n",
      "[58]\tTest-merror:0.145749\n",
      "[59]\tTest-merror:0.144849\n",
      "[60]\tTest-merror:0.145299\n",
      "[61]\tTest-merror:0.14395\n",
      "[62]\tTest-merror:0.144849\n",
      "[63]\tTest-merror:0.144399\n",
      "[64]\tTest-merror:0.144849\n",
      "[65]\tTest-merror:0.14305\n",
      "[66]\tTest-merror:0.14305\n",
      "[67]\tTest-merror:0.1426\n",
      "[68]\tTest-merror:0.14215\n",
      "[69]\tTest-merror:0.14215\n",
      "[70]\tTest-merror:0.14215\n",
      "[71]\tTest-merror:0.140801\n",
      "[72]\tTest-merror:0.140801\n",
      "[73]\tTest-merror:0.139901\n",
      "[74]\tTest-merror:0.139451\n",
      "[75]\tTest-merror:0.139451\n",
      "[76]\tTest-merror:0.139451\n",
      "[77]\tTest-merror:0.137202\n",
      "[78]\tTest-merror:0.137652\n",
      "[79]\tTest-merror:0.137202\n",
      "[80]\tTest-merror:0.135852\n",
      "[81]\tTest-merror:0.134503\n",
      "[82]\tTest-merror:0.134053\n",
      "[83]\tTest-merror:0.134953\n",
      "[84]\tTest-merror:0.133603\n",
      "[85]\tTest-merror:0.133603\n",
      "[86]\tTest-merror:0.133603\n",
      "[87]\tTest-merror:0.133153\n",
      "[88]\tTest-merror:0.132254\n",
      "[89]\tTest-merror:0.131804\n",
      "[90]\tTest-merror:0.130454\n",
      "[91]\tTest-merror:0.130454\n",
      "[92]\tTest-merror:0.130454\n",
      "[93]\tTest-merror:0.130454\n",
      "[94]\tTest-merror:0.130454\n",
      "[95]\tTest-merror:0.130454\n",
      "[96]\tTest-merror:0.130004\n",
      "[97]\tTest-merror:0.130004\n",
      "[98]\tTest-merror:0.129105\n",
      "[99]\tTest-merror:0.130004\n",
      "[100]\tTest-merror:0.129105\n",
      "[101]\tTest-merror:0.128655\n",
      "[102]\tTest-merror:0.128655\n",
      "[103]\tTest-merror:0.128655\n",
      "[104]\tTest-merror:0.128655\n",
      "[105]\tTest-merror:0.127755\n",
      "[106]\tTest-merror:0.126856\n",
      "[107]\tTest-merror:0.126856\n",
      "[108]\tTest-merror:0.126406\n",
      "[109]\tTest-merror:0.127305\n",
      "[110]\tTest-merror:0.126406\n",
      "[111]\tTest-merror:0.126406\n",
      "[112]\tTest-merror:0.126406\n",
      "[113]\tTest-merror:0.126406\n",
      "[114]\tTest-merror:0.126856\n",
      "[115]\tTest-merror:0.126856\n",
      "[116]\tTest-merror:0.127305\n",
      "[117]\tTest-merror:0.126406\n",
      "[118]\tTest-merror:0.126406\n",
      "[119]\tTest-merror:0.126406\n",
      "[120]\tTest-merror:0.125506\n",
      "[121]\tTest-merror:0.126406\n",
      "[122]\tTest-merror:0.125056\n",
      "[123]\tTest-merror:0.125056\n",
      "[124]\tTest-merror:0.125056\n",
      "[125]\tTest-merror:0.125056\n",
      "[126]\tTest-merror:0.125956\n",
      "[127]\tTest-merror:0.125506\n",
      "[128]\tTest-merror:0.125506\n",
      "[129]\tTest-merror:0.125056\n",
      "[130]\tTest-merror:0.125956\n",
      "[131]\tTest-merror:0.124606\n",
      "[132]\tTest-merror:0.124606\n",
      "[133]\tTest-merror:0.124606\n",
      "[134]\tTest-merror:0.123707\n",
      "[135]\tTest-merror:0.123707\n",
      "[136]\tTest-merror:0.123707\n",
      "[137]\tTest-merror:0.123707\n",
      "[138]\tTest-merror:0.123707\n",
      "[139]\tTest-merror:0.122807\n",
      "[140]\tTest-merror:0.122807\n",
      "[141]\tTest-merror:0.122807\n",
      "[142]\tTest-merror:0.122807\n",
      "[143]\tTest-merror:0.122357\n",
      "[144]\tTest-merror:0.123257\n",
      "[145]\tTest-merror:0.123257\n",
      "[146]\tTest-merror:0.122807\n",
      "[147]\tTest-merror:0.123257\n",
      "[148]\tTest-merror:0.123257\n",
      "[149]\tTest-merror:0.123257\n",
      "[150]\tTest-merror:0.122807\n",
      "[151]\tTest-merror:0.122357\n",
      "[152]\tTest-merror:0.122807\n",
      "[153]\tTest-merror:0.123257\n",
      "[154]\tTest-merror:0.123257\n",
      "[155]\tTest-merror:0.123257\n",
      "[156]\tTest-merror:0.122807\n",
      "[157]\tTest-merror:0.121907\n",
      "[158]\tTest-merror:0.122807\n",
      "[159]\tTest-merror:0.122807\n",
      "[160]\tTest-merror:0.122807\n",
      "[161]\tTest-merror:0.122807\n",
      "[162]\tTest-merror:0.122807\n",
      "[163]\tTest-merror:0.122807\n",
      "[164]\tTest-merror:0.123257\n",
      "[165]\tTest-merror:0.123257\n",
      "[166]\tTest-merror:0.122357\n",
      "[167]\tTest-merror:0.122357\n",
      "[168]\tTest-merror:0.122357\n",
      "[169]\tTest-merror:0.121907\n",
      "[170]\tTest-merror:0.121907\n",
      "[171]\tTest-merror:0.122357\n",
      "[172]\tTest-merror:0.122357\n",
      "[173]\tTest-merror:0.122357\n",
      "[174]\tTest-merror:0.122357\n",
      "[175]\tTest-merror:0.121907\n",
      "[176]\tTest-merror:0.122357\n",
      "[177]\tTest-merror:0.122807\n",
      "Stopping. Best iteration:\n",
      "[157]\tTest-merror:0.121907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params['eta'] = 0.05\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    xg_train,\n",
    "    num_boost_round = num_boost_round,\n",
    "    evals = [(xg_test,\"Test\")],\n",
    "    early_stopping_rounds =20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytest = np.reshape(y_test,(len(X_test), 100))\n",
    "pred = model.predict(xg_test)\n",
    "error_rate = np.sum(np.argmax(pred) != np.argmax(y_test))/np.shape(y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22., 62., 79., ...,  5., 48., 58.], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_test.get_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [222300, 2223]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-dc5ac1ce6aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGetEER_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c2bed413cf52>\u001b[0m in \u001b[0;36mGetEER_\u001b[0;34m(y_score, y_test)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmissRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [222300, 2223]"
     ]
    }
   ],
   "source": [
    "GetEER_(pred,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%cal` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
